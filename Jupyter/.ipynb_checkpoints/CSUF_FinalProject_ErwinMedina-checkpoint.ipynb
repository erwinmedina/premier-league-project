{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c6d44fd-d5ba-45da-b62c-860e1e863d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f56ebf4d-ff32-48af-8101-6751291fc3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- #\n",
    "# Step 1: Load the data #\n",
    "# --------------------- #\n",
    "df = pd.read_excel(\"MasterDataset_ML.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e5b7667-a427-4e06-bb62-dd3f2fd81dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------- #\n",
    "# Step 1.1: Remove referees that have a count less than a certain value #\n",
    "# --------------------------------------------------------------------- #\n",
    "counts = df['Referee'].value_counts()\n",
    "rare_refs = counts[counts < 70].index\n",
    "df = df[~df['Referee'].isin(rare_refs)]\n",
    "\n",
    "# ----------------------------------- #\n",
    "# Step 2: Define targets and features #\n",
    "# ----------------------------------- #\n",
    "Target = 'Referee'\n",
    "# Target = 'WIN'\n",
    "Feature = 'Referee' if Target == 'WIN' else 'WIN'\n",
    "\n",
    "# --------------------------------------------------------------- #\n",
    "# Step 2.1: If Target = \"WIN\", then remove the goal score columns #\n",
    "# --------------------------------------------------------------- #\n",
    "if (Target==\"WIN\"):\n",
    "    df = df.drop('H_Score', axis=1)\n",
    "    df = df.drop('A_Score', axis=1)\n",
    "\n",
    "df = df.drop(['Stadium Capacity', 'MatchId'], axis=1) # Might not need this but keep for now..\n",
    "\n",
    "y = df[Target] \n",
    "x = df.drop(Target, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "894be25c-b498-4e98-8129-9e6b079400a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- # \n",
    "# Step 3: Label encode the target # \n",
    "# ------------------------------- # \n",
    "le = LabelEncoder() \n",
    "y_encoded = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0daf94a0-1faf-429b-a586-ac58116761cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------- # \n",
    "# Step 4: One hot encode key categorical columns # \n",
    "# ---------------------------------------------- # \n",
    "categorial_columns = [\"Home\", \"Away\", \"Stadium City\", \"Referee - UK Region of Birth\", \"Season Start Year\", Feature] \n",
    "x_encoded = pd.get_dummies(x, columns=categorial_columns, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44150947-f3fe-4962-9b68-9726a56491e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use mean strategy with non-numeric data:\ncould not convert string to float: '2009-PL-002'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (Simple):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# IMPUTE\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     imputer \u001b[38;5;241m=\u001b[39m SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     x_imputed \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(imputer\u001b[38;5;241m.\u001b[39mfit_transform(x_encoded), columns\u001b[38;5;241m=\u001b[39mx_encoded\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# SCALE\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    904\u001b[0m             (\n\u001b[1;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    914\u001b[0m         )\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/impute/_base.py:434\u001b[0m, in \u001b[0;36mSimpleImputer.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the imputer on `X`.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 434\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_input(X, in_fit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;66;03m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;66;03m# otherwise\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/impute/_base.py:361\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[0;34m(self, X, in_fit)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not convert\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n\u001b[1;32m    356\u001b[0m     new_ve \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m strategy with non-numeric data:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    358\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy, ve\n\u001b[1;32m    359\u001b[0m         )\n\u001b[1;32m    360\u001b[0m     )\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_ve \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ve\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use mean strategy with non-numeric data:\ncould not convert string to float: '2009-PL-002'"
     ]
    }
   ],
   "source": [
    "# ------------------------- #\n",
    "# Step 5: Imputer & Scaling #\n",
    "# ------------------------- #\n",
    "Simple = True\n",
    "\n",
    "# -------------- #\n",
    "# SIMPLE IMPUTER #\n",
    "# -------------- #\n",
    "if (Simple):\n",
    "    # IMPUTE\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    x_imputed = pd.DataFrame(imputer.fit_transform(x_encoded), columns=x_encoded.columns)\n",
    "\n",
    "    # SCALE\n",
    "    scaler = StandardScaler()\n",
    "    x_scaled = scaler.fit_transform(x_imputed)\n",
    "    \n",
    "# ----------- #\n",
    "# KNN IMPUTER #\n",
    "# ----------- #\n",
    "else :\n",
    "    # SCALE\n",
    "    scaler = StandardScaler()\n",
    "    x_scaled = scaler.fit_transform(x_encoded)\n",
    "    x_scaled_df = pd.DataFrame(x_scaled, columns=x_encoded.columns)\n",
    "\n",
    "    # IMPUTE\n",
    "    imputer = KNNImputer(n_neighbors=5, weights=\"distance\")\n",
    "    x_imputed = pd.DataFrame(imputer.fit_transform(x_scaled_df), columns=x_scaled_df.columns)\n",
    "\n",
    "# The x value that goes into train_test_split\n",
    "TTS_xValue = x_scaled if Simple == True else x_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91d91fe-cf7e-43cd-bc03-1f56b8ab03fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ #\n",
    "# Step 6: Split train/test #\n",
    "# ------------------------ #\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    TTS_xValue, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc2387-ac1b-439a-bb19-fcd6bb31241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- #\n",
    "# Step 7: Train Random Forest #\n",
    "# --------------------------- #\n",
    "if Target == \"Referee\":\n",
    "    rf = RandomForestClassifier(class_weight=\"balanced\", max_depth= 20, min_samples_leaf= 4, min_samples_split= 2, n_estimators= 200, random_state=42)\n",
    "\n",
    "# --------------------------------------------------- #\n",
    "# If the target was which side won [home, away, draw] #\n",
    "# --------------------------------------------------- #\n",
    "else:\n",
    "    weights = {\n",
    "        0: 1.1,\n",
    "        1: 1.4,\n",
    "        2: 0.7,\n",
    "    }\n",
    "    rf = RandomForestClassifier(class_weight=weights, max_depth= 30, min_samples_leaf= 4, min_samples_split= 10, n_estimators= 300, random_state=42)\n",
    "\n",
    "rf.fit(x_train, y_train)\n",
    "y_pred = rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c8c1ed-86ba-4981-8ec4-9f85ab1302d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# Step 8 - Evaluate Performance #\n",
    "# ----------------------------- #\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_encoded, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9f838d-1638-43d6-bb66-89169964ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------- #\n",
    "# Step 9 - Feature Importance (Detection Bias) #\n",
    "# -------------------------------------------- #\n",
    "importances = rf.feature_importances_\n",
    "features = x_train.columns\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(\"\\nTop 30 Most Influential Features:\")\n",
    "print(importance_df.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5bded1-b075-4795-8efa-7e7bc31ef8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- #\n",
    "# Step 10 - Visualize it! #\n",
    "# ----------------------- #\n",
    "federation_importance = importance_df[importance_df['Feature'].str.contains(\"AFC|CAF|UEFA|CONMEBOL|CONCACAF|OFC\")]\n",
    "print(\"\\nFederation Feature Importance\")\n",
    "print(federation_importance)\n",
    "\n",
    "grouped_importance = importance_df.groupby(\n",
    "    importance_df['Feature'].apply(\n",
    "        lambda x: 'Federation' if any(f in x for f in [\"AFC\", \"CAF\", \"UEFA\", \"CONMEBOL\", \"CONCACAF\", \"OFC\"])\n",
    "        else 'RefereeBirthCity' if 'Referee - UK Region of Birth' in x\n",
    "        else 'MatchStats'\n",
    "    )\n",
    ")['Importance'].sum()\n",
    "print(\"\\nGrouped Feature Importances:\")\n",
    "print(grouped_importance)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=importance_df.head(15), x='Importance', y='Feature')\n",
    "plt.title('Top 15 Features Influencing Referee Predictions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76513bb-a7fe-4ced-9f07-efd1d21d2bd9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------ #\n",
    "# Step 3: Train/Test Split #\n",
    "# ------------------------ #\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df.drop('Referee', axis=1),\n",
    "    df['Referee'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['Referee']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002caf2-270f-4bc2-a76f-82732b8721ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------- #\n",
    "# Step 4: Compute Referee Averages #\n",
    "# -------------------------------- #\n",
    "train_df = x_train.copy()\n",
    "train_df['Referee'] = y_train\n",
    "\n",
    "ref_stats_train = train_df.groupby('Referee').agg({\n",
    "    'Total_Yellow_Cards': 'mean',\n",
    "    'Total_Red_Cards': 'mean',\n",
    "    'Total_Fouls': 'mean',\n",
    "    'PenaltiesAwarded': 'mean',\n",
    "    # 'MatchId': 'count',\n",
    "# }).rename(columns={'MatchId': 'matches_officiated'})\n",
    "})\n",
    "\n",
    "ref_stats_train['diciplinary_index'] = (\n",
    "    ref_stats_train['Total_Yellow_Cards'] \n",
    "    + 2 * ref_stats_train['Total_Red_Cards'] \n",
    "    + 0.5 * ref_stats_train['PenaltiesAwarded']\n",
    ")\n",
    "\n",
    "# Merge these averages into both splits\n",
    "x_train = x_train.copy()\n",
    "x_test = x_test.copy()\n",
    "x_train['Referee'] = y_train\n",
    "x_test['Referee'] = y_test\n",
    "\n",
    "# Merge based on that temporary column\n",
    "x_train = x_train.merge(ref_stats_train, left_on='Referee', right_index=True, how='left')\n",
    "x_test = x_test.merge(ref_stats_train, left_on='Referee', right_index=True, how='left')\n",
    "x_test.fillna(ref_stats_train.mean(), inplace=True)\n",
    "# print(x_train)\n",
    "\n",
    "# Drop the Referee column again + matchId\n",
    "x_train = x_train.drop(columns=['Referee'])\n",
    "x_train = x_train.drop(columns=['MatchId'])\n",
    "x_test = x_test.drop(columns=['Referee'])\n",
    "x_test = x_test.drop(columns=['MatchId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc26a636-be7d-424a-ac2d-5f4602b80fc1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------- #\n",
    "# Step 5: Encode the target  #\n",
    "# -------------------------- #\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d59c57e-2e74-44e3-9e09-79695cc37214",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------- #\n",
    "# Step 6: One-hot encode categorical features  #\n",
    "# -------------------------------------------- #\n",
    "categorical_columns = [\n",
    "    \"Home\", \"Away\", \"Stadium City\",\n",
    "    \"Referee - UK Region of Birth\", \"Season Start Year\", Feature\n",
    "]\n",
    "\n",
    "# Fit encoder only on training data\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "encoder.fit(x_train[categorical_columns])\n",
    "\n",
    "# Transform both sets\n",
    "x_train_encoded = pd.DataFrame(\n",
    "    encoder.transform(x_train[categorical_columns]),\n",
    "    columns=encoder.get_feature_names_out(categorical_columns),\n",
    "    index=x_train.index\n",
    ")\n",
    "x_test_encoded = pd.DataFrame(\n",
    "    encoder.transform(x_test[categorical_columns]),\n",
    "    columns=encoder.get_feature_names_out(categorical_columns),\n",
    "    index=x_test.index\n",
    ")\n",
    "\n",
    "# Merge back encoded + numeric columns\n",
    "x_train = pd.concat([x_train.drop(columns=categorical_columns), x_train_encoded], axis=1)\n",
    "x_test = pd.concat([x_test.drop(columns=categorical_columns), x_test_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e881de04-60d5-4f3c-979d-875e844837c6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------- #\n",
    "# Step 7: Impute & Scale    #\n",
    "# ------------------------- #\n",
    "Simple = True\n",
    "\n",
    "if Simple:\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    x_train_imputed = pd.DataFrame(imputer.fit_transform(x_train), columns=x_train.columns, index=x_train.index)\n",
    "    x_test_imputed = pd.DataFrame(imputer.transform(x_test), columns=x_test.columns, index=x_test.index)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = pd.DataFrame(scaler.fit_transform(x_train_imputed), columns=x_train.columns, index=x_train.index)\n",
    "    x_test_scaled = pd.DataFrame(scaler.transform(x_test_imputed), columns=x_test.columns, index=x_test.index)\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = pd.DataFrame(scaler.fit_transform(x_train), columns=x_train.columns, index=x_train.index)\n",
    "    x_test_scaled = pd.DataFrame(scaler.transform(x_test), columns=x_test.columns, index=x_test.index)\n",
    "\n",
    "    imputer = KNNImputer(n_neighbors=5, weights=\"distance\")\n",
    "    x_train_scaled = pd.DataFrame(imputer.fit_transform(x_train_scaled), columns=x_train.columns, index=x_train.index)\n",
    "    x_test_scaled = pd.DataFrame(imputer.transform(x_test_scaled), columns=x_test.columns, index=x_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd526cf-c398-4426-829e-5bfa6ac3756f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------- #\n",
    "# Step 8: Train Random Forest #\n",
    "# --------------------------- #\n",
    "if Target == \"Referee\":\n",
    "    rf = RandomForestClassifier(\n",
    "        class_weight=\"balanced\",\n",
    "        max_depth=20,\n",
    "        min_samples_leaf=4,\n",
    "        min_samples_split=2,\n",
    "        n_estimators=200,\n",
    "        random_state=42\n",
    "    )\n",
    "else:\n",
    "    weights = {0: 1.1, 1: 1.4, 2: 0.7}\n",
    "    rf = RandomForestClassifier(\n",
    "        class_weight=weights,\n",
    "        max_depth=30,\n",
    "        min_samples_leaf=4,\n",
    "        min_samples_split=10,\n",
    "        n_estimators=300,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "rf.fit(x_train_scaled, y_train_encoded)\n",
    "y_pred = rf.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9563e5bd-06e0-4801-a651-ee9d2ce39ec3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # ------------------- #\n",
    "# # Step 8: ParamGrid ! #\n",
    "# # ------------------- #\n",
    "# rf = RandomForestClassifier(random_state=42)\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100,200,300],\n",
    "#     'max_depth': [None, 10,20,30],\n",
    "#     'min_samples_split': [2,5,10],\n",
    "#     'min_samples_leaf': [1,2,4],\n",
    "#     'class_weight': [None, 'balanced']\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator = rf,\n",
    "#     param_grid = param_grid,\n",
    "#     cv = 3,\n",
    "#     n_jobs = -1,\n",
    "#     verbose = 2\n",
    "# )\n",
    "\n",
    "# grid_search.fit(x_train, y_train)\n",
    "# print(\"Best Parameters: \", grid_search.best_params_)\n",
    "# print(\"Best Accuracy: \", grid_search.best_score_)\n",
    "\n",
    "# best_rf = grid_search.best_estimator_\n",
    "# y_pred = best_rf.predict(x_test)\n",
    "# print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5fd77-ff6b-422a-a604-65265c53d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- #\n",
    "# Step 12 - Multiple Models! #\n",
    "# -------------------------- #\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000), # Add regularization to the parameters\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(max_depth= 20, min_samples_leaf= 4, min_samples_split= 2, n_estimators= 200, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"SVM\": SVC(),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=20) # Adjust, make this smaller depending\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results.append({\"Model\": name, \"Accuracy\": acc})\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"Accuracy\", ascending=False)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02db8363-df9d-44a3-ba9a-58f75f716b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- #\n",
    "# Visualize multiple models! #\n",
    "# -------------------------- #\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=results_df, x=\"Accuracy\", y=\"Model\")\n",
    "plt.title(\"Model Accuracy Comparison\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec250ce-d736-4f19-b84e-328f00f5f0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
